<link rel="stylesheet" href="style.css">

<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Spring 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">

  <base href="." target="_blank"></head>

<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Minority Languages: Sourashtra</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Audi Quattro</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
          </div>
          <p>    
              Amoligha Timma
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Gehad Abdelrahman
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              Malak Raafat
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
          </div>
          <p>
            Abhi Bijlwan
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="CSCI_5541_AudiQuattro_FinalReport.pdf"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>             
        </div>
      </div>

    </div>
  </div>

  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Minority languages like Sourashtra are largely

    underrepresented in Natural Language Processing (NLP), making tasks like sentiment analysis challenging due to limited training data.
    
    Since multilingual transformer models such
    as BERT and XLM-RoBERTa rely on large
    datasets, they often struggle with low resource
    
    languages. This project explores whether finetuning XLM-RoBERTa on linguistically related
    
    Indian languages (e.g., Gujarati, Marathi) improves sentiment classification for Sourashtra.
    
    To evaluate this, we create a new Sourashtra
    sentiment dataset and compare performance
    
    across models fine-tuned on related and unrelated languages. Our findings contribute to
    
    ongoing research in low-resource NLP, high-
    lighting the role of linguistic similarity in crosslingual transfer learning and providing a framework for improving NLP applications for underrepresented languages.
</p>

<hr>

<h2 id="introduction">Introduction</h2>

<p>
    Languages play an important role in preserving

and recognizing cultural identity. Yet minority languages like Sourashtra remain vastly underrepresented in Natural Language Processing research

and implementations. Due to limited amounts of
text based data, most popular models struggle to
process and understand minority languages. As
a result of this, key NLP tasks such as sentiment
analysis, translation, and text generation remain

inaccessible to communities that speak these languages.

The limited existence of Sourashtra (and similar minority languages) in the digital space creates

major challenges for computational language processes. Unlike widely spoken languages such as

English and Hindi, Sourashtra doesn’t have a large scale system for lingual data. This makes it difficult

to develop models that understand the language’s

unique grammar, phonetics, and sentiments. Existing pre-trained models such as BERT and mBERT,

perform poorly on minority languages in comparison to languages like Chinese. This is due to the

model’s reliance on large, diverse training datasets.
This exclusion not only limits NLP advancements

but also restricts accessibility for Sourashtra speakers in areas such as education, translations, and

even social media.

</p>
<p>
To bridge the gap between Sourashtra and large

LLMs, our project proposes to fine-tune a multilingual pre-trained model on influential Indian
languages such as Hindi, Gujarati, Marathi, Telugu, Tamil, and Malayalam. These languages have
had major influences on the Sourashtra language
and are thus vital for our training. By leveraging
these related languages with large datasets, we aim
to improve the cross-lingual transfer learning and
enhance sentiment classification for the language.
The model will then be evaluated and tested on a

custom-made dataset. This will be designed specifically for the Sourashtra language to assess its ability to generalize and adapt to minority vernaculars.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
We began by using the capable multilingual model XLM-RoBERTa to address the issue of limited data in minority languages like Sourashtra. 
First, we gathered large datasets in related Indian languages like Hindi, Gujarati, Marathi, Telugu, Tamil, and Malayalam that possess richer textual resources.
By fine-tuning the model on these languages, we enabled it to learn common linguistic and sentiment patterns that could be potentially transferred to or be seen in Sourashtra.

Next, we further fine-tuning the model on a custom-made Sourashtra sentiment dataset. 
Our strategy is rooted in the theory that linguistic similarities among the 6 popular Indian languages would allow the model to successfully capture the linguistic nuances of Sourashtra. 
The innovative aspect of our approach lies in a two method traing: first gathering abundant data from related languages and then adapting that knowledge to a low-resource language. 
This method not only improves sentiment classification accuracy for Sourashtra but also demonstrates a novel, scalable pathway for enhancing NLP applications in other minority languages.

</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
One major problem that we anticipated and faced was retrieving a sufficient dataset for the Sourashtra language. Since it is a minority language, there are not many abundant resources/resources that
we could utilize. To overcome this, we had one of our group members create a custom datasheet, as they are a native speaker. This provided us with approximately 250 positive and negative sentiments 
that we fed to the pre-trained XLM-RoBERTa model. 
</p>

<hr>

<h2 id="results">Results</h2>

<section id="quantitative-results">
  <h3>Quantitative Results</h3>
  <p>Table 1 shows the accuracy of each model on our Sourashtra test set.</p>
  <table>
    <thead>
      <tr>
        <th>Model</th>
        <th>Accuracy (%)</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>Baseline (XLM-RoBERTa)</td><td>50.0</td></tr>
      <tr><td>Gujarati fine-tuned</td><td>50.4</td></tr>
      <tr><td>Marathi fine-tuned</td><td>56.8</td></tr>
      <tr><td>Hindi fine-tuned</td><td>52.4</td></tr>
      <tr><td>Malayalam fine-tuned</td><td>49.6</td></tr>
      <tr><td>Tamil fine-tuned</td><td>50.0</td></tr>
      <tr><td>Telugu fine-tuned</td><td>56.8</td></tr>
      <tr><td>Indo-Aryan ensemble</td><td>53.2</td></tr>
      <tr><td>Dravidian ensemble</td><td>50.8</td></tr>
      <tr><td>Sourashtra fine-tuned</td><td>64.0</td></tr>
      <tr><td>Augmented Gujarati</td><td>65.4</td></tr>
      <tr><td>Augmented Marathi</td><td>66.3</td></tr>
      <tr><td>Augmented Hindi</td><td>69.2</td></tr>
      <tr><td>Augmented Indo-Aryan ensemble</td><td>67.1</td></tr>
    </tbody>
  </table>
</section>

<section id="sample-size-results">
  <h3>Sample-Size Sensitivity</h3>
  <p>Table 2 shows how varying the number of Sourashtra samples in the augmented Hindi model affects accuracy.</p>
  <table>
    <thead>
      <tr>
        <th>Number of Sourashtra Samples</th>
        <th>Accuracy (%)</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>2</td><td>50.0</td></tr>
      <tr><td>4</td><td>50.0</td></tr>
      <tr><td>6</td><td>50.0</td></tr>
      <tr><td>8</td><td>50.0</td></tr>
      <tr><td>10</td><td>63.8</td></tr>
      <tr><td>12</td><td>50.0</td></tr>
      <tr><td>14</td><td>50.0</td></tr>
      <tr><td>16</td><td>63.3</td></tr>
      <tr><td>18</td><td>50.0</td></tr>
      <tr><td>20</td><td>65.3</td></tr>
    </tbody>
  </table>
</section>

<hr>

<h2 id="conclusion">Conclusion and Future Work</h2>

<p>
Our experiments show that fine-tuning XLM-RoBERTa on linguistically related Indo-Aryan languages delivers modest gains over the 50 % baseline (Gujarati 50.4 %, Marathi 56.8 %, Hindi 52.4 %) and that simple ensembles of these languages reach 53.2 % accuracy. Fully fine-tuning on our 250-sentence Sourashtra dataset boosts accuracy to 64.0 %. Crucially, targeted data augmentation—injecting just 10 Sourashtra sentences into each fine-tuning run—yields further improvements: augmented Gujarati 65.4 %, augmented Marathi 66.3 %, augmented Hindi 69.2 %, and an augmented Indo-Aryan ensemble at 67.1 % accuracy. These results demonstrate that even minimal in-language data can substantially enhance cross-lingual transfer, improving the model’s grasp of context and negation beyond surface lexical overlap.
</p>

<p>
However, we observed significant variability in performance depending on which examples were selected for augmentation, indicating that not all samples contribute equally to generalization. Additionally, sensitivity to learning rates and training duration suggests that more robust or adaptive optimization strategies may be necessary to stabilize results.
</p>

<p>
Looking ahead, we plan to:
<ul>
  <li>Expand and diversify our Sourashtra sentiment corpus in collaboration with native speakers and community stakeholders to capture richer linguistic and cultural nuance </li>
  <li>Investigate principled sample-selection methods—such as active learning or curriculum learning—to identify the most informative sentences for augmentation </li>
  <li>Explore fine-tuning on Dravidian languages historically influencing Sourashtra, and experiment with alternative model architectures (e.g., adapter modules or additional neural layers) to uncover new performance gains.</li>
  <li>Introduce multiple annotators and perform detailed error analyses on failure cases (negation, sarcasm, ambiguous phrasing) to guide targeted improvements.</li>
  <li>Evaluate our models on downstream tasks—such as machine translation or topic classification—to assess broader applicability and robustness for low-resource language technologies.</li>
</ul>
</p>

<hr>
  </div>

</body></html>
