<link rel="stylesheet" href="style.css">

<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Spring 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">

  <base href="." target="_blank"></head>

<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Minority Languages: Sourashtra</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Audi Quattro</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
          </div>
          <p>    
              Amoligha Timma
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Gehad Abdelrahman
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              Malak Raafat
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
          </div>
          <p>
            Abhi Bijlwan
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="CSCI_5541_AudiQuattro_FinalReport.pdf"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="Final_Training_Updated_Audi_Quattro_Project.zip"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Project Code</span>
            </a>
          </span>
        </div>
      </div>

    </div>
  </div>

  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Minority languages like Sourashtra are largely

    underrepresented in Natural Language Processing (NLP), making tasks like sentiment analysis challenging due to limited training data.
    
    Since multilingual transformer models such
    as BERT and XLM-RoBERTa rely on large
    datasets, they often struggle with low resource
    
    languages. This project explores whether finetuning XLM-RoBERTa on linguistically related
    
    Indian languages (e.g., Gujarati, Marathi) improves sentiment classification for Sourashtra.
    
    To evaluate this, we create a new Sourashtra
    sentiment dataset and compare performance
    
    across models fine-tuned on related and unrelated languages. Our findings contribute to
    
    ongoing research in low-resource NLP, high-
    lighting the role of linguistic similarity in crosslingual transfer learning and providing a framework for improving NLP applications for underrepresented languages.
</p>

<hr>

<h2 id="introduction">Introduction</h2>

<p>
    Languages play an important role in preserving

and recognizing cultural identity. Yet minority languages like Sourashtra remain vastly underrepresented in Natural Language Processing research

and implementations. Due to limited amounts of
text based data, most popular models struggle to
process and understand minority languages. As
a result of this, key NLP tasks such as sentiment
analysis, translation, and text generation remain

inaccessible to communities that speak these languages.

The limited existence of Sourashtra (and similar minority languages) in the digital space creates

major challenges for computational language processes. Unlike widely spoken languages such as

English and Hindi, Sourashtra doesn’t have a large scale system for lingual data. This makes it difficult

to develop models that understand the language’s

unique grammar, phonetics, and sentiments. Existing pre-trained models such as BERT and mBERT,

perform poorly on minority languages in comparison to languages like Chinese. This is due to the

model’s reliance on large, diverse training datasets.
This exclusion not only limits NLP advancements

but also restricts accessibility for Sourashtra speakers in areas such as education, translations, and

even social media.

</p>
<p>
To bridge the gap between Sourashtra and large

LLMs, our project proposes to fine-tune a multilingual pre-trained model on influential Indian
languages such as Hindi, Gujarati, Marathi, Telugu, Tamil, and Malayalam. These languages have
had major influences on the Sourashtra language
and are thus vital for our training. By leveraging
these related languages with large datasets, we aim
to improve the cross-lingual transfer learning and
enhance sentiment classification for the language.
The model will then be evaluated and tested on a

custom-made dataset. This will be designed specifically for the Sourashtra language to assess its ability to generalize and adapt to minority vernaculars.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
Our goal was to build a robust sentiment‐analysis pipeline for Sourashtra, a low-resource minority language with virtually no off-the-shelf NLP tools. We chose XLM-RoBERTa as our starting point because its broad multilingual pretraining offers a strong foundation—but without adaptation, it typically yields only chance‐level performance on underrepresented tongues. Drawing on historical linguistics, we hypothesized that fine-tuning on closely related Indo-Aryan languages (Gujarati, Marathi, Hindi) would steer the model towards the morphosyntactic and semantic patterns of Sourashtra. At the same time, we contrasted this with fine-tuning on unrelated Dravidian languages (Tamil, Telugu, Malayalam) to empirically validate the importance of genuine linguistic affinity rather than mere dataset volume.
</p>

<p>
To put this into practice, we first curated a brand-new, balanced 250-sentence Sourashtra sentiment dataset (125 positive, 125 negative) entirely from scratch, ensuring each example preserved authentic grammar and vocabulary without English intrusions. We then conducted three sequential experiments:
<ul>
  <li> We fine-tuned XLM-RoBERTa separately on each source language dataset, measuring how much cross-lingual transfer each afforded.</li>
  <li> Recognizing early that raw transfer delivered only modest gains, we augmented each Indo-Aryan training run with a tiny batch of 10 Sourashtra examples (5 pos / 5 neg). This minimal in-language “seed data” was intended to anchor the model’s decision boundaries to genuine Sourashtra usage.</li>
  <li> Finally, we combined the three best Indo-Aryan fine-tuned models into a simple majority‐vote ensemble, aiming to leverage complementary strengths and reduce individual model variance.</li>
</ul>
By layering these steps, we moved from chance performance (50.0%) to sustained improvements: moderate boosts from base fine-tuning, substantial jumps after augmentation (up to ≈ 69%), and a final ensemble peak at 67.08%.
</p>

<p>
The journey surfaced several deep challenges:
<ul>
  <li> Public sentiment corpora for our source languages varied dramatically in size, annotation schemes, and domain. Harmonizing these datasets required extensive cleaning, re-labeling, and normalization to avoid introducing noise during transfer.</li>
  <li> Crafting our Sourashtra corpus was a labor-intensive, one-person effort. Without multiple annotators, we struck a balance between linguistic purity and topical diversity, yet some syntactic structures remain underrepresented.</li>
  <li> Early single-language fine-tuning yielded only incremental gains—sometimes under 2 %—and one Dravidian run even underperformed the baseline. This underscored that raw data volume alone cannot substitute for true linguistic alignment.</li>
  <li> We found that small changes in learning rate or epoch count could swing results by several points, indicating that low-resource fine-tuning demands more stable, adaptive training protocols.</li>
</ul>
These insights drove our pivot toward highly targeted data augmentation and motivate future work on principled sample-selection (e.g., active learning or informativeness scoring), adaptive optimization schedules, and the exploration of additional related languages (including Dravidian influences) to further elevate performance.
</p>

<hr>

<h2 id="results">Results</h2>

<!-- Paragraph 1: Overall model accuracies -->
<p>
We started with a baseline XLM-RoBERTa at 50.0% accuracy (no fine-tuning). Single-language fine-tuning yielded modest gains—Gujarati (50.4%), Marathi (56.8%), Hindi (52.4%), Malayalam (49.6 %), Tamil (50.0 %), Telugu (56.8 %)—and simple ensembles of Indo-Aryan (Gujarati + Marathi + Hindi) reached 53.2 %, while the Dravidian ensemble (Tamil + Telugu + Malayalam) reached 50.8 %. Fine-tuning fully on our 250-sentence Sourashtra set jumped accuracy to 64.0 %, and injecting just 10 Sourashtra examples into each Indo-Aryan model (augmented experiments) further improved performance to 65.42 % (Gujarati), 66.25 % (Marathi), 69.17 % (Hindi) and 67.08 % for the combined augmented Indo-Aryan ensemble.
</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Accuracy (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Baseline (no fine-tuning)</td><td>50.0</td></tr>
    <tr><td>Gujarati fine-tuned</td><td>50.4</td></tr>
    <tr><td>Marathi fine-tuned</td><td>56.8</td></tr>
    <tr><td>Hindi fine-tuned</td><td>52.4</td></tr>
    <tr><td>Malayalam fine-tuned</td><td>49.6</td></tr>
    <tr><td>Tamil fine-tuned</td><td>50.0</td></tr>
    <tr><td>Telugu fine-tuned</td><td>56.8</td></tr>
    <tr><td>Indo-Aryan ensemble</td><td>53.2</td></tr>
    <tr><td>Dravidian ensemble</td><td>50.8</td></tr>
    <tr><td>Sourashtra fine-tuned</td><td>64.0</td></tr>
    <tr><td>Augmented Gujarati</td><td>65.42</td></tr>
    <tr><td>Augmented Marathi</td><td>66.25</td></tr>
    <tr><td>Augmented Hindi</td><td>69.17</td></tr>
    <tr><td>Augmented Indo-Aryan ensemble</td><td>67.08</td></tr>
  </tbody>
</table>

<!-- Paragraph 2: Class-level performance -->
<p>
On class-level metrics, our fully fine-tuned Sourashtra model (64.0% overall) achieved a strong recall on negative examples (0.72) but lower recall on positive examples (0.50), yielding F1 = 0.67 for negatives. In contrast, the Augmented Indo-Aryan ensemble (61.0% overall) excelled at positives (recall = 0.84) but missed most negatives (recall = 0.32), giving F1 = 0.42 on the negative class.
</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Accuracy</th>
      <th>Recall (Positive)</th>
      <th>Recall (Negative)</th>
      <th>F1 (Positive)</th>
      <th>F1 (Negative)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Fully fine-tuned Sourashtra</td>
      <td>64.0%</td>
      <td>0.50</td>
      <td>0.72</td>
      <td>0.57</td>
      <td>0.67</td>
    </tr>
    <tr>
      <td>Augmented Indo-Aryan ensemble</td>
      <td>61.0%</td>
      <td>0.84</td>
      <td>0.32</td>
      <td>0.62</td>
      <td>0.42</td>
    </tr>
  </tbody>
</table>

<!-- Paragraph 3: Sample-size sensitivity -->
<p>
To probe data-augmentation sensitivity, we varied the number of Sourashtra examples (2–20) in our best Augmented Hindi model (LR = 3e-5, batch = 16, 7 epochs). Results were highly inconsistent: most runs hovered at 50 %, but peaks appeared at 10 (63.8 %), 16 (63.3 %) and 20 (65.3 %) samples. Re-sampling the same sizes frequently dropped performance back to 50 %, underlining that specific sentences carry disproportionately informative cues and motivating more principled sample-selection strategies.
</p>

<table>
  <thead>
    <tr>
      <th># Sourashtra Samples</th>
      <th>Accuracy (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>2</td><td>50.0</td></tr>
    <tr><td>4</td><td>50.0</td></tr>
    <tr><td>6</td><td>50.0</td></tr>
    <tr><td>8</td><td>50.0</td></tr>
    <tr><td>10</td><td>63.75</td></tr>
    <tr><td>12</td><td>50.0</td></tr>
    <tr><td>14</td><td>50.0</td></tr>
    <tr><td>16</td><td>63.25</td></tr>
    <tr><td>18</td><td>50.0</td></tr>
    <tr><td>20</td><td>65.25</td></tr>
  </tbody>
</table>

<hr>

<h2 id="conclusion">Conclusion and Future Work</h2>

<p>
Our experiments show that fine-tuning XLM-RoBERTa on linguistically related Indo-Aryan languages delivers modest gains over the 50% baseline (Gujarati 50.4%, Marathi 56.8%, Hindi 52.4%) and that simple ensembles of these languages reach 53.2% accuracy. Fully fine-tuning on our 250-sentence Sourashtra dataset boosts accuracy to 64.0%. Crucially, targeted data augmentation—injecting just 10 Sourashtra sentences into each fine-tuning run—yields further improvements: augmented Gujarati 65.4%, augmented Marathi 66.3%, augmented Hindi 69.2%, and an augmented Indo-Aryan ensemble at 67.1% accuracy. These results demonstrate that even minimal in-language data can substantially enhance cross-lingual transfer, improving the model’s grasp of context and negation beyond surface lexical overlap.
</p>

<p>
However, we observed significant variability in performance depending on which examples were selected for augmentation, indicating that not all samples contribute equally to generalization. Additionally, sensitivity to learning rates and training duration suggests that more robust or adaptive optimization strategies may be necessary to stabilize results.
</p>

<p>
Looking ahead, we plan to:
<ul>
  <li>Expand and diversify our Sourashtra sentiment corpus in collaboration with native speakers and community stakeholders to capture richer linguistic and cultural nuance </li>
  <li>Investigate principled sample-selection methods—such as active learning or curriculum learning—to identify the most informative sentences for augmentation </li>
  <li>Explore fine-tuning on Dravidian languages historically influencing Sourashtra, and experiment with alternative model architectures (e.g., adapter modules or additional neural layers) to uncover new performance gains.</li>
  <li>Introduce multiple annotators and perform detailed error analyses on failure cases (negation, sarcasm, ambiguous phrasing) to guide targeted improvements.</li>
  <li>Evaluate our models on downstream tasks—such as machine translation or topic classification—to assess broader applicability and robustness for low-resource language technologies.</li>
</ul>
</p>

<hr>
  </div>

</body></html>
